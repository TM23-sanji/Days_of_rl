{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bf7ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ptan gymnasium torch torchvision pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2407467f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "import typing as tt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.001\n",
    "ENTROPY_BETA = 0.01\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "REWARD_STEPS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0577c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGN(nn.Module):\n",
    "    def __init__(self, input_size: int, n_actions: int):\n",
    "        super(PGN, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb84ff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(old: tt.Optional[float], val: float, alpha: float = 0.95) -> float:\n",
    "    if old is None:\n",
    "        return val\n",
    "    return old * alpha + (1. - alpha) * val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4ded0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "writer = SummaryWriter(comment=\"-cartpole-pg\")\n",
    "\n",
    "net = PGN(env.observation_space.shape[0], env.action_space.n)\n",
    "print(net)\n",
    "\n",
    "agent = ptan.agent.PolicyAgent(\n",
    "    net, preprocessor=ptan.agent.float32_preprocessor,\n",
    "    apply_softmax=True)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "        env, agent, gamma=GAMMA, steps_count=REWARD_STEPS)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "total_rewards = []\n",
    "step_rewards = []\n",
    "step_idx = 0\n",
    "done_episodes = 0\n",
    "reward_sum = 0.0\n",
    "bs_smoothed = entropy = l_entropy = l_policy = l_total = None\n",
    "\n",
    "batch_states, batch_actions, batch_scales = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6b6048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
