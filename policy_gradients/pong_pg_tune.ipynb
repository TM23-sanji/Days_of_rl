{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e542e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "import argparse\n",
    "import collections, sys, time, numpy as np, typing as tt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils as nn_utils\n",
    "import torch.optim as optim\n",
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37b4be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardTracker:\n",
    "    def __init__(self, writer, stop_reward):\n",
    "        self.writer = writer\n",
    "        self.stop_reward = stop_reward\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.ts = time.time()\n",
    "        self.ts_frame = 0\n",
    "        self.total_rewards = []\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *args):\n",
    "        self.writer.close()\n",
    "    \n",
    "    def reward(self, reward, frame, epsilon=None):\n",
    "        self.total_rewards.append(reward)\n",
    "        speed = (frame - self.ts_frame) / (time.time() - self.ts)\n",
    "        self.ts_frame = frame\n",
    "        self.ts = time.time()\n",
    "        mean_reward = np.mean(self.total_rewards[-100:])\n",
    "        print(\"%d: done %d games, mean reward %.3f, speed %.2f f/s%s\" % (\n",
    "            frame, len(self.total_rewards), mean_reward, speed, epsilon_str\n",
    "        ))\n",
    "        sys.stdout.flush()\n",
    "        if epsilon is not None:\n",
    "            self.writer.add_scalar(\"epsilon\", epsilon, frame)\n",
    "        self.writer.add_scalar(\"speed\", speed, frame)\n",
    "        self.writer.add_scalar(\"reward_100\", mean_reward, frame)\n",
    "        self.writer.add_scalar(\"reward\", reward, frame)\n",
    "        if mean_reward > self.stop_reward:\n",
    "            print(\"Solved in %d frames!\" % frame)\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d892ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtariPGN(nn.Module):\n",
    "    def __init__(self, input_shape: tt.Tuple(int, ...), n_actions: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        size = self.conv(torch.zeros(1, *input_shape)).size()[-1]\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.ByteTensor) -> torch.Tensor:\n",
    "        xx = x/255.0\n",
    "        x = self.conv(xx)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23939fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "#LEARNING_RATE = 0.0001\n",
    "#ENTROPY_BETA = 0.02\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "#REWARD_STEPS = 10\n",
    "BASELINE_STEPS = 1000000\n",
    "#GRAD_L2_CLIP = 0.1\n",
    "EVAL_STEPS = 1_000_000\n",
    "\n",
    "ENV_COUNT = 32\n",
    "\n",
    "PARAMS_SPACE = {\n",
    "    \"lr\": tune.loguniform(1e-5, 1e-3),\n",
    "    \"reward_steps\": tune.choice([3, 5, 7, 9]),\n",
    "    \"grad_clip\": tune.loguniform(1e-2, 1),\n",
    "    \"beta\": tune.loguniform(1e-4, 1e-1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad48a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env():\n",
    "    return ptan.common.wrappers.wrap_dqn(gym.make(\"PongNoFrameskip-v4\"))\n",
    "\n",
    "class MeanBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.capacity = capacity\n",
    "        self.deque = collections.deque(maxlen=capacity)\n",
    "        self.sum = 0.0\n",
    "\n",
    "    def add(self, val: float):\n",
    "        if len(self.deque) == self.capacity:\n",
    "            self.sum -= self.deque[0]\n",
    "        self.deque.append(val)\n",
    "        self.sum += val\n",
    "\n",
    "    def mean(self) -> float:\n",
    "        if not self.deque:\n",
    "            return 0.0\n",
    "        return self.sum / len(self.deque)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e3eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config: dict, device: torch.device) -> dict:\n",
    "    p_lr = config[\"p_lr\"]\n",
    "    p_reward_steps = config[\"p_reward_steps\"]\n",
    "    p_grad_clip = config[\"p_grad_clip\"]\n",
    "    p_beta = config[\"p_beta\"]\n",
    "\n",
    "    envs = [make_env() for _ in range(ENV_COUNT)]\n",
    "\n",
    "    net = AtariPGN(envs[0].observation_space.shape, envs[0].action_space.n).to(device)\n",
    "    print(net)\n",
    "\n",
    "    agent = ptan.agent.PolicyAgent(net, apply_softmax=True, device=device)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(envs, agent, gamma=GAMMA, steps_count=p_reward_steps)\n",
    "\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=p_lr, eps=1e-3)\n",
    "\n",
    "    train_step_idx = 0\n",
    "    baseline_buf = MeanBuffer(BASELINE_STEPS)\n",
    "    reward_buf = MeanBuffer(100)\n",
    "\n",
    "    batch_states, batch_actions, batch_scales = [], [], []\n",
    "    max_reward = None\n",
    "\n",
    "    for step_idx, exp in enumerate(exp_source):\n",
    "        if step_idx > EVAL_STEPS:\n",
    "            break\n",
    "        baseline_buf.add(exp.reward)\n",
    "        baseline = baseline_buf.mean()\n",
    "        batch_states.append(exp.state)\n",
    "        batch_actions.append(exp.action)\n",
    "        batch_scales.append(exp.reward - baseline)\n",
    "\n",
    "        new_rewards = exp_source.pop_total_rewards()\n",
    "        if new_rewards:\n",
    "            for r in new_rewards:\n",
    "                reward_buf.add(r)\n",
    "            max_rw = reward_buf.mean()\n",
    "            if max_reward is None or max_rw > max_reward:\n",
    "                print(f\"{step_idx}: Max mean reward updated: {max_reward} -> {max_rw:.2f}\")\n",
    "                max_reward = max_rw\n",
    "        if len(batch_states) < BATCH_SIZE:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdffa6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step_idx += 1\n",
    "states_v = torch.as_tensor(np.asarray(batch_states)).to(device)\n",
    "batch_actions_t = torch.as_tensor(batch_actions).to(device)\n",
    "batch_scale_v = torch.as_tensor(batch_scales).to(device)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "logits_v = net(states_v)\n",
    "log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "log_prob_actions_v = batch_scale_v * log_prob_v[range(BATCH_SIZE), batch_actions_t]\n",
    "loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "prob_v = F.softmax(logits_v, dim=1)\n",
    "entropy_v = -(prob_v * log_prob_v).sum(dim=1).mean()\n",
    "entropy_loss_v = -p_beta * entropy_v\n",
    "loss_v = loss_policy_v + entropy_loss_v\n",
    "loss_v.backward()\n",
    "nn_utils.clip_grad_norm_(net.parameters(), p_grad_clip)\n",
    "optimizer.step()\n",
    "\n",
    "batch_states.clear()\n",
    "batch_actions.clear()\n",
    "batch_scales.clear()\n",
    "\n",
    "for e in envs:\n",
    "    e.close()\n",
    "return {\"max_reward\": max_reward}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4942414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\n",
    "#         \"--dev\", default=\"cpu\", help=\"Device to use, default=cpu\")\n",
    "#     parser.add_argument(\"--samples\", type=int, default=20, help=\"Count of samples to run\")\n",
    "#     args = parser.parse_args()\n",
    "#     device = torch.device(args.dev)\n",
    "\n",
    "#     config = tune.TuneConfig(num_samples=args.samples)\n",
    "#     obj = tune.with_parameters(train, device=device)\n",
    "#     if device.type == 'cuda':\n",
    "#         obj = tune.with_resources(obj, {\"gpu\": 1})\n",
    "#     tuner = tune.Tuner(\n",
    "#         obj, param_space=PARAMS_SPACE, tune_config=config\n",
    "#     )\n",
    "#     results = tuner.fit()\n",
    "#     best = results.get_best_result(metric=\"max_reward\", mode=\"max\")\n",
    "#     print(best.config)\n",
    "#     print(best.metrics)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
